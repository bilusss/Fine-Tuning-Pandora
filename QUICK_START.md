# ğŸš€ Szybki Start - Fine-Tuning Mistral-7B

## 1ï¸âƒ£ Podstawowe uruchomienie (ZALECANE)

```bash
# Aktywuj Å›rodowisko wirtualne (jeÅ›li potrzeba)
source .venv/bin/activate

# Uruchom trening
.venv/bin/python run.py
```

## 2ï¸âƒ£ Z logowaniem do pliku

```bash
./run_safe.sh
```

## 3ï¸âƒ£ Ultra-safe mode (jeÅ›li problemy z pamiÄ™ciÄ…)

```bash
.venv/bin/python run_ultra_safe.py
```

## ğŸ“Š Monitorowanie podczas treningu

### Terminal 1: Trening
```bash
.venv/bin/python run.py
```

### Terminal 2: Monitor GPU
```bash
watch -n 1 nvidia-smi
```

### Terminal 3: Monitor procesu (opcjonalnie)
```bash
watch -n 5 'ps aux | grep python | grep run.py'
```

## â±ï¸ Oczekiwany czas

| Wersja | Dataset | Czas |
|--------|---------|------|
| `run.py` | 50k | **15-22h** |
| `run_ultra_safe.py` | 50k | **20-25h** |

## ğŸ“ WyjÅ›ciowe pliki

```
./Mistral7b01-qlora-36h/          â† Model z run.py
./Mistral7b01-qlora-ultrasafe/    â† Model z run_ultra_safe.py
training_log_*.txt                 â† Logi (jeÅ›li uÅ¼ywasz run_safe.sh)
```

## ğŸ” Sprawdzenie statusu w trakcie

```bash
# SprawdÅº ostatnie linie logu
tail -f training_log_*.txt

# SprawdÅº uÅ¼ycie GPU
nvidia-smi

# SprawdÅº checkpointy
ls -lah ./Mistral7b01-qlora-36h/
```

## ğŸ›‘ Zatrzymanie treningu

```bash
# Gracefully (Ctrl+C w terminalu)
# Lub znajdÅº proces:
ps aux | grep python | grep run.py
kill -15 <PID>  # Graceful stop
```

## ğŸ”„ Wznowienie z checkpointu

JeÅ›li trening zostaÅ‚ przerwany, moÅ¼esz wznowiÄ‡ z ostatniego checkpointu.

Zmodyfikuj `run.py`:
```python
# Zamiast:
trainer.train()

# UÅ¼yj:
trainer.train(resume_from_checkpoint=True)
```

## âš ï¸ Troubleshooting

### Problem: Out of Memory
```bash
# UÅ¼yj ultra-safe mode
.venv/bin/python run_ultra_safe.py

# Lub zmniejsz batch size w run.py
BATCH_SIZE = 2
GRADIENT_ACCUMULATION_STEPS = 8
```

### Problem: Wolny trening
```bash
# SprawdÅº czy GPU jest uÅ¼ywane
nvidia-smi

# SprawdÅº prÄ™dkoÅ›Ä‡
# Powinno byÄ‡ ~2-3s per step dla run.py
# JeÅ›li wolniej, sprawdÅº:
# - Czy inne procesy nie uÅ¼ywajÄ… GPU
# - Czy dataloader_num_workers nie jest za duÅ¼y
```

### Problem: Brak miejsca na dysku
```bash
# UsuÅ„ stare checkpointy
rm -rf ./Mistral7b01-qlora-finetuned/checkpoint-*
rm -rf ./llama3-finedtuned-big5/

# Checkpointy zajmujÄ… ~500MB kaÅ¼dy
```

## ğŸ“¦ Po zakoÅ„czeniu

### Testowanie modelu
```python
from peft import AutoPeftModelForCausalLM
import torch

model = AutoPeftModelForCausalLM.from_pretrained(
    './Mistral7b01-qlora-36h',
    device_map='auto',
    dtype=torch.float16
)

# UÅ¼yj modelu...
```

### Merge LoRA adaptera z modelem bazowym (opcjonalnie)
```python
merged_model = model.merge_and_unload()
merged_model.save_pretrained('./Mistral7b01-merged')
```

## ğŸ’¾ Wymagania dyskowe

- Model bazowy: ~13GB
- Checkpoint: ~500MB
- Dataset cache: ~2GB
- **ÅÄ…cznie**: ~20GB wolnego miejsca

---

## ğŸ¯ Optymalne ustawienia dla Twojej karty

**RTX 5070 Ti 16GB** - `run.py`:
- âœ… Batch size: 4
- âœ… Gradient accumulation: 4
- âœ… Sequence length: 128
- âœ… Dataset: 50k przykÅ‚adÃ³w
- âœ… Czas: ~15-22h
- âœ… VRAM peak: ~10-12GB
